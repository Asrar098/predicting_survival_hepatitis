{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Basic Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load data into pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hepatitis_data = pd.read_csv(\"dataset_55_hepatitis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction algorithm**  \n",
    "\n",
    "One of the main decisions to make when performing machine learning is choosing the appropriate algorithm that fits the current problem we are dealing with.\n",
    "**Supervised learning** refers to the task of inferring a function from a labeled training dataset. We fit the model to the labeled training set with the main goal of finding the optimal parameters that will predict unknown labels of new examples included in the test dataset. There are two main types of supervised learning: regression, in which we want to predict a label that is a real number, and classification, in which we want to predict a categorical label.\n",
    "In our case, we have a labeled dataset and we want to use a classification algorithm to find the label in the categorical values: 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find many classification supervised learning algorithms, some simple but efficient, such as linear classifier or logistic regression, and another ones more complex but powerful such as decision trees and k-means.  \n",
    "In this case, we will choose **Random Forest** algorithm. Random forest is one of the most used machine learning algorithm due to the fact that it is very simple, flexible and easy to use but produces reliable results.\n",
    "So we will load the packages from scikit-learn that we need to perform Random Forest and also to evaluate afterwards the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {'no': 0,\n",
    "               'yes': 1,\n",
    "               'DIE': 0,\n",
    "               'LIVE': 1,\n",
    "               '?': np.nan,\n",
    "               'female': 0,\n",
    "               'male': 1}\n",
    "\n",
    "hepatitis_data.replace(replacements, inplace = True)\n",
    "hepatitis_data = hepatitis_data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EDA, we dropped all `NaN` values. Here, we need to evaluate what is the best method to handle them.  \n",
    "There are several ways to deal with missing data but none of them is perfect. The first step is to understand why data went missing. In our case, we can guess that the values missing in the categorical variables could be due to the absence of the feature that instead of being imputed as 'no' was left blank or that it was not tested. Also, missing values in continuous variables could be explained by the lack of biochemical studies performed in that particular patient or because the parameters were within normal range and it was not written down.  \n",
    "\n",
    "In both cases, we could be in the presence of **Missing at Random** value (The fact that the value is missing has nothing to do with the hypothetical value) or **Missing not at Random** value (The missing value depends on the hypothetical value). If it was the first one, we could drop the `NaN` value safely, while in the last case it would not be safe to drop it because this missing value tell us something about the hypothetical value. So we will then impute the values of the missing value once we are about to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE                 0\n",
       "SEX                 0\n",
       "STEROID             1\n",
       "ANTIVIRALS          0\n",
       "FATIGUE             1\n",
       "MALAISE             1\n",
       "ANOREXIA            1\n",
       "LIVER_BIG          10\n",
       "LIVER_FIRM         11\n",
       "SPLEEN_PALPABLE     5\n",
       "SPIDERS             5\n",
       "ASCITES             5\n",
       "VARICES             5\n",
       "BILIRUBIN           6\n",
       "ALK_PHOSPHATE      29\n",
       "SGOT                4\n",
       "ALBUMIN            16\n",
       "PROTIME            67\n",
       "HISTOLOGY           0\n",
       "Class               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hepatitis_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Test datasets**\n",
    "\n",
    "In order to train and test our model, we need to split our dataset into to subdatasets, the training and the test dataset. The model will learn from the training dataset to generalize to other data; the test dataset will be used to \"test\" what the model learnt in the training and fitting step. \n",
    "It is common to use the rule of 80%-20% to split the original dataset. It is important to use a reliable method to split the dataset to avoid data leakage; this is the presence in the test set of examples that were also in the training set. \n",
    "First, we will assign all the columns except our dependant variable (\"Class\") to the variable X and the column \"Class\" to the variable Y.\n",
    "And then we will `train_test_split` from the scikit-learn library to split them into X_train, X_test, Y_train and Y_test. It is important to add `random_state` because this will allow us to have the same results every time we run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hepatitis_data.iloc[:, hepatitis_data.columns != 'Class']\n",
    "y = hepatitis_data.iloc[:, hepatitis_data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training Random Forest **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy now to impute missing values (using Imputer), create and train the basic random forest model using the package Scikit-learn.\n",
    "We will start by apply .ravel() to the Y_train and Y_test to flatten our array as not doing so will rise warnings from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.values.ravel()\n",
    "Y_test = Y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will impute our missing values using the function `Imputer` and the strategy `most_frequent` that will replace the missing values for the most frequent value in the column (axis = 0). It is worthy to notice that doing so can introduce errors and bias, but of course as we state before there is no perfect way to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values = 'NaN', strategy = \"most_frequent\", axis = 0)\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "fit_random_forest = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "fit_random_forest.fit(X_train_imp, Y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic model has now been trained and has learnt the relationship between our independent variables and the target variable. Now, we can check how good our model is by making predictions on the test set. We can then compare the prediction with our known labels.\n",
    "\n",
    "We will again impute the missing values in our test set and use the function `predict` and the metrics `accuracy_score` to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imp = imp.transform(X_test)\n",
    "\n",
    "y_predicted = fit_random_forest.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74193548387096775"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe above, our basic model has an accuracy of 74.19% which tell us that it has to be further improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
