{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Basic Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load data into pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hepatitis_data = pd.read_csv(\"dataset_55_hepatitis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction algorithm**  \n",
    "\n",
    "One of the main decisions to make when performing machine learning is choosing the appropriate algorithm that fits the current problem we are dealing with.\n",
    "**Supervised learning** refers to the task of inferring a function from a labeled training dataset. We fit the model to the labeled training set with the main goal of finding the optimal parameters that will predict unknown labels of new examples included in the test dataset. There are two main types of supervised learning: regression, in which we want to predict a label that is a real number, and classification, in which we want to predict a categorical label.\n",
    "In our case, we have a labeled dataset and we want to use a classification algorithm to find the label in the categorical values: 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find many classification supervised learning algorithms, some simple but efficient, such as linear classifier or logistic regression, and another ones more complex but powerful such as decision trees and k-means.  \n",
    "In this case, we will choose **Random Forest** algorithm. Random forest is one of the most used machine learning algorithm due to the fact that it is very simple, flexible and easy to use but produces reliable results.\n",
    "So we will load the packages from scikit-learn that we need to perform Random Forest and also to evaluate afterwards the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacements = {'no': 0,\n",
    "               'yes': 1,\n",
    "               'DIE': 0,\n",
    "               'LIVE': 1,\n",
    "               '?': np.nan,\n",
    "               'female': 0,\n",
    "               'male': 1}\n",
    "\n",
    "hepatitis_data.replace(replacements, inplace = True)\n",
    "hepatitis_data = hepatitis_data.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the EDA, we dropped all `NaN` values. Here, we need to evaluate what is the best method to handle them.  \n",
    "There are several ways to deal with missing data but none of them is perfect. The first step is to understand why data went missing. In our case, we can guess that the values missing in the categorical variables could be due to the absence of the feature that instead of being imputed as 'no' was left blank or that it was not tested. Also, missing values in continuous variables could be explained by the lack of biochemical studies performed in that particular patient or because the parameters were within normal range and it was not written down.  \n",
    "\n",
    "In both cases, we could be in the presence of **Missing at Random** value (The fact that the value is missing has nothing to do with the hypothetical value) or **Missing not at Random** value (The missing value depends on the hypothetical value). If it was the first one, we could drop the `NaN` value safely, while in the last case it would not be safe to drop it because this missing value tell us something about the hypothetical value. So we will then impute the values of the missing value once we are about to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE                 0\n",
       "SEX                 0\n",
       "STEROID             1\n",
       "ANTIVIRALS          0\n",
       "FATIGUE             1\n",
       "MALAISE             1\n",
       "ANOREXIA            1\n",
       "LIVER_BIG          10\n",
       "LIVER_FIRM         11\n",
       "SPLEEN_PALPABLE     5\n",
       "SPIDERS             5\n",
       "ASCITES             5\n",
       "VARICES             5\n",
       "BILIRUBIN           6\n",
       "ALK_PHOSPHATE      29\n",
       "SGOT                4\n",
       "ALBUMIN            16\n",
       "PROTIME            67\n",
       "HISTOLOGY           0\n",
       "Class               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hepatitis_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Test datasets**\n",
    "\n",
    "In order to train and test our model, we need to split our dataset into to subdatasets, the training and the test dataset. The model will learn from the training dataset to generalize to other data; the test dataset will be used to \"test\" what the model learnt in the training and fitting step. \n",
    "It is common to use the rule of 80%-20% to split the original dataset. It is important to use a reliable method to split the dataset to avoid data leakage; this is the presence in the test set of examples that were also in the training set. \n",
    "First, we will assign all the columns except our dependant variable (\"Class\") to the variable X and the column \"Class\" to the variable Y.\n",
    "And then we will `train_test_split` from the scikit-learn library to split them into X_train, X_test, Y_train and Y_test. It is important to add `random_state` because this will allow us to have the same results every time we run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = hepatitis_data.iloc[:, hepatitis_data.columns != 'Class']\n",
    "y = hepatitis_data.iloc[:, hepatitis_data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training Random Forest **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy now to impute missing values (using Imputer), create and train the basic random forest model using the package Scikit-learn.\n",
    "We will start by apply .ravel() to the Y_train and Y_test to flatten our array as not doing so will rise warnings from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = Y_train.values.ravel()\n",
    "Y_test = Y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will impute our missing values using the function `Imputer` and the strategy `most_frequent` that will replace the missing values for the most frequent value in the column (axis = 0). It is worthy to notice that doing so can introduce errors and bias, but of course as we state before there is no perfect way to handle missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values = 'NaN', strategy = \"most_frequent\", axis = 0)\n",
    "imp = imp.fit(X_train)\n",
    "\n",
    "X_train_imp = imp.transform(X_train)\n",
    "\n",
    "fit_random_forest = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "fit_random_forest.fit(X_train_imp, Y_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic model has now been trained and has learnt the relationship between our independent variables and the target variable. Now, we can check how good our model is by making predictions on the test set. We can then compare the prediction with our known labels.\n",
    "\n",
    "We will again impute the missing values in our test set and use the function `predict` and the metrics `accuracy_score` to evaluate the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_imp = imp.transform(X_test)\n",
    "\n",
    "y_predicted = fit_random_forest.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.19 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, y_predicted)*100\n",
    "print(round(accuracy, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe above, our basic model has an accuracy of 74.19% which tell us that it has to be further improved. \n",
    "\n",
    "** Hyperparameters tuning** \n",
    "\n",
    "There are several ways to improve our model: gather more data, tune the hyperparameters of the model or choose other models. We will choose the second one, we will now tune the hyperparameters of our random forest classifier.\n",
    "\n",
    "Model parameters are normally learned during training; however hyperparameters must be set manually before training. In the case of random forest, hyperparameters include:\n",
    "\n",
    "* n_estimators: number of trees in the forest\n",
    "* max_features: maximum number of features in each tree\n",
    "* max_depth: maximum splits for all trees\n",
    "* bootstrap: whether to implement bootstrap or not to build trees\n",
    "* criterion: assess stopping criteria for decision trees\n",
    "\n",
    "Of course, when we implement basic random forest, Scikit-learn implements a set of default hyperparameters, but we are not sure if those parameters are the optimal for our particular problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this point is when we need to considered two concepts: underfitting and overfitting. *Underfitting* occurs when the model is too simple and it doesn't fit the data well: it has low variance but high bias. On the other hand, *overfitting* occurs when the model adjust too well to the training set and performs poorly in new examples.\n",
    "If we tune the hyperparameters in the training dataset, we would then be prone to overfit our random forest classifier. So instead, we will go back to what was mentioned before: the *cross validation*.  \n",
    "We will use **K-fold cross validation** method to tune the hyperparameters: we will perform many iterations on the K-subset cross validation but using different model settings each time. Afterwards, we compare all models and select the best one; then, we will train the best model in the full training set and evaluate it on the testing set. We will take advantage of *GridSearchCV* package in Scikit-learn to perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will determine the parameters and values that we want to optimize and then we will performed the GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_optimize = {\n",
    "                      'max_features': ['auto', 'sqrt', 'log2', None], \n",
    "                      'max_depth': [2,3, 4],\n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'bootstrap': [True, False],\n",
    "                      'n_estimators': [2, 5, 10, 15, 20]\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_hyp = RandomForestClassifier()\n",
    "\n",
    "random_forest_search = GridSearchCV(random_forest_hyp,\n",
    "                                   cv = 20,\n",
    "                                   param_grid = parameters_optimize,\n",
    "                                   n_jobs = 3)\n",
    "random_forest_search.fit(X_train_imp, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameteres after GridSearchCV {'bootstrap': True, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameteres after GridSearchCV', random_forest_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set our model with the best parameters given by the GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_hyp.set_params(bootstrap = True, \n",
    "                            criterion = 'gini',\n",
    "                            max_depth = 4,\n",
    "                            max_features = 'sqrt',\n",
    "                            n_estimators = 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_hyp.fit(X_train_imp, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted_grid = random_forest_hyp.predict(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.42 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_grid = accuracy_score(Y_test, y_predicted_grid)*100\n",
    "print(round(accuracy_grid, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above our GridSearchCV improve our accuracy from 74 to 77%. Even though that it's not a great improvement, it has been reported that with this dataset other reports reach only an accuracy of 80%. So considering this and the fact that the dataset has many missing data and it's not big (only 155 samples) we can go on and analyse other model metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
